{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "매개 변수가 너무 많습니다 - 2\n"
     ]
    }
   ],
   "source": [
    "!tree -dL 2 data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_train의 타입: <class 'list'>\n",
      "text_train의 길이: 25000\n",
      "text_train[6]:\n",
      " b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"C:\\/Users/iws1/Text/aclImdb/train/\")\n",
    "# 텍스트와 레이블을 포함하고 있는 Bunch 오브젝트를 반환합니다.\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"text_train의 타입:\", type(text_train))\n",
    "print(\"text_train의 길이:\", len(text_train))\n",
    "print(\"text_train[6]:\\n\", text_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수 (훈련 데이터): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"클래스별 샘플 수 (훈련 데이터):\", np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 문서 수: 25000\n",
      "클래스별 샘플 수 (테스트 데이터): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"C:\\/Users/iws1/Text/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"테스트 데이터의 문서 수:\", len(text_test))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터):\", np.bincount(y_test))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words = [\"The fool doth think he is wise,\", \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 13\n",
      "어휘 사전의 내용:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기:\", len(vect.vocabulary_))\n",
    "print(\"어휘 사전의 내용:\\n\", vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"BOW:\", repr(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW의 밀집 표현:\n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW의 밀집 표현:\\n\", bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " <25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n\", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 개수: 74849\n",
      "처음 20개 특성:\n",
      " ['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "20010에서 20030까지 특성:\n",
      " ['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "매 2000번째 특성:\n",
      " ['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"특성 개수:\", len(feature_names))\n",
    "print(\"처음 20개 특성:\\n\", feature_names[:20])\n",
    "print(\"20010에서 20030까지 특성:\\n\", feature_names[20010:20030])\n",
    "print(\"매 2000번째 특성:\\n\", feature_names[::2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증 평균 점수: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "print(\"교차 검증 평균 점수: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 교차 검증 점수: 0.89\n",
      "최적의 매개변수: {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 교차 검증 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 매개변수:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"테스트 점수: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"min_df로 제한한 X_train:\", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"처음 50개 특성:\\n\", feature_names[:50])\n",
    "print(\"20,010부터 20,030까지 특성:\\n\", feature_names[20010:20030])\n",
    "print(\"매 700번째 특성:\\n\", feature_names[::700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최적의 교차 검증 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"불용어 개수:\", len(ENGLISH_STOP_WORDS))\n",
    "print(\"매 10번째 불용어:\\n\", list(ENGLISH_STOP_WORDS)[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words=\"english\"라고 지정하면 내장된 불용어를 사용합니다.\n",
    "# 내장된 불용어에 추가할 수도 있고 자신만의 목록을 사용할 수도 있습니다.\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"불용어가 제거된 X_train:\\n\", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 교차 검증 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=5000))\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"최상이 교차 검증 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# 훈련 데이터셋을 변환합니다.\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# 특성별로 가장 큰 값을 찾습니다.\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# 특성 이름을 구합니다.\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"가장 낮은 tfidf를 가진 특성:\\n\", feature_names[sorted_by_tfidf[:20]])\n",
    "print(\"가장 높은 tfidf를 가진 특성:\\n\", feature_names[sorted_by_tfidf[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"가장 낮은 idf를 가진 특성:\\n\", feature_names[sorted_by_idf[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps[\"logisticregression\"].coef_[0],\n",
    "                                    feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bards_words:\\n\", bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"어휘 사전 크기:\", len(cv.vocabulary_))\n",
    "print(\"어휘 사전:\\n\", cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"어휘 사전 크기:\", len(cv.vocabulary_))\n",
    "print(\"어휘 사전:\\n\", cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"변환된 데이터 (밀집 배열):\\n\", cv.transform(bards_words).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"어휘 사전 크기:\", len(cv.vocabulary_))\n",
    "print(\"어휘 사전:\\n\", cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=5000))\n",
    "# 매개변수 조합이 많고 트라이그램이 포함되어 있기 때문에\n",
    "# 그리드 서치 실행에 시간이 오래 걸립니다.\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"최상의 교차 검증 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 매개변수:\\n\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드 서치에서 테스트 점수를 추출합니다.\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
    "# 히트맵을 그립니다.\n",
    "heatmap = mglearn.tools.heatmap(scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n",
    "                               xticklabels=param_grid['logisticregression__C'],\n",
    "                                 yticklabels=param_grid['tfidfvectorizer__ngram_range'])\n",
    "plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 이름과 계수를 추출합니다.\n",
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "coef = grid.best_estimator_.named_steps['logisticregression'].coef_\n",
    "mglearn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트라이그램 특성을 찾습니다.\n",
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "# 트라이그램 특성만 그래프로 나타냅니다.\n",
    "# mglearn.tools.visualize_coefficients(coef.ravel()[mask], feature_names[mask], n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy 버전 3.0.6\n",
      "nltk 버전 3.5\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(\"spacy 버전\", spacy.__version__)\n",
    "import nltk\n",
    "print(\"nltk 버전\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy의 영어 모델을 로드합니다.\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "# nltk의 PorterStemmer 객체를 만듭니다.\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# spacy의 표제어 추출과 nltk의 어간 추출을 비교하는 함수입니다.\n",
    "def compare_normalization(doc):\n",
    "    # spacy로 문서를 토큰화합니다.\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    # spacy로 찾은 표제어를 출력합니다.\n",
    "    print(\"표제어:\")\n",
    "    print([token.lemma_ for token in doc_spacy])\n",
    "    # PorterStemmer로 찾은 토큰을 출력합니다.\n",
    "    print(\"어간:\")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어:\n",
      "['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'I', 'be', 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "어간:\n",
      "['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', 'am', 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "compare_normalization(u\"Our meeting today was worse than yesterday, \" \"I'm scared of meeting the clients tomorrow.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요구사항: spacy에서 표제어 추출 기능과 CountVectorzier의 토큰 분할기를 사용합니다.\n",
    "\n",
    "# spacy의 언어 모델을 로드합니다.\n",
    "en_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# spacy 문서 처리 파이프라인을 사용해 자작 토큰 분할기를 만듭니다.\n",
    "# (우리만의 토큰 분할기를 사용합니다.)\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# 자작 토큰 분할기를 사용해 CountVectorizer 객체를 만듭니다.\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출이 가능한 CountVectorizer 객체로 text_train을 변환합니다.\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape:\", X_train_lemma.shape)\n",
    "\n",
    "#비교를 위해 표준 CountVectorizer를 사용합니다.\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 세트의 1%만 사용해서 그리드 서치를 만듭니다.\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99, train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, cv=cv)\n",
    "\n",
    "# 기본 CountVectorizer로 그리드 서치를 수행합니다.\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 교차 검증 점수\" \"(기본 CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "\n",
    "# 표제어를 사용해서 그리드 서치를 수행합니다.\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(\"최상의 교차 검증 점수 \" \"(표제어): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:\\/Users/iws1/Text/nsmc-master/ratings_train.txt', delimiter='\\t', keep_default_na=False)\n",
    "df_train.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train['document'].values, df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('C:\\/Users/iws1/Text/nsmc-master/ratings_test.txt', delimiter='\\t', keep_default_na=False)\n",
    "text_test = df_test['document'].values\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_train), np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_test), np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt_tag = Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okt_tokenizer(text):\n",
    "    return okt_tag.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "okt_param_grid = {'tfidfvectorizer__min_df': [3, 5, 7],\n",
    "                 'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                 'logisticregression__C': [0.1, 1, 10]}\n",
    "okt_pipe = make_pipeline(TfidfVectorizer(tokenizer=okt_tokenizer), LogisticRegression(solver='liblinear'))\n",
    "okt_grid = GridSearchCV(okt_pipe, okt_param_grid, cv=3)\n",
    "\n",
    "# 그리드 서치를 수행합니다.\n",
    "okt_grid.fit(text_train[0:1000], y_train[0:1000])\n",
    "print(\"최상의 교차 검증 점수: {:.3f}\".format(okt_grid.best_score_))\n",
    "print(\"최적이 교차 검증 매개변수: \", format(okt_grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
